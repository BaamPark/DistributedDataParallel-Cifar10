# Distributed Data Parallel Training with PyTorch
This repository contains Python scripts for training a PyTorch model using Distributed Data Parallel (DDP). 
DDP is a parallel and distributed training technique that can significantly reduce training time by leveraging multiple GPUs across multiple nodes. 
The provided Python script demonstrates how to set up and use DDP for training a defined ResNet model on CIFAR10 dataset.

